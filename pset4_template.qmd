---
title: "Problem-set-4"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
PRVDR_CTGRY_SBTYP_CD (Provider Category Subtype Code)
PRVDR_CTGRY_CD (Provider Category Code)
PRVDR_NUM (CMS Certification Number)
FAC_NAME (Facility Name)
ZIP_CD (Address: Zip Code)
PGM_TRMNTN_CD (Terpmination Code)
TRMNTN_EXPRTN_DT (Termination Expiration Date)
STATE_CD (State Code)

2. 
    a.
    ```{python}
    
    import pandas as pd

    file_path_2016 = '~/Desktop/problem-set-4-xy-wz/pos2016.csv'
    pos2016 = pd.read_csv(file_path_2016)
    
    short_term_hospitals_2016 = pos2016[(pos2016['PRVDR_CTGRY_CD'] == 1) & (pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1)]

    num_hospitals_2016 = short_term_hospitals_2016.shape[0]
    print(f"Number of short-term hospitals in 2016: {num_hospitals_2016}")

    ```

    This number appears reasonable given that the CMS dataset is known to include all hospitals eligible to bill Medicare.

    b.The number of short-term hospitals reported in the CMS POS dataset for 2016 is 7,245.  However, the Agency for Healthcare Research and Quality (AHRQ) reported 4,661 short-term acute care hospitals for the same year.  This difference might be because the CMS dataset includes all hospitals eligible to bill Medicare, while the AHRQ data only includes hospitals that meet specific criteria, like operating for at least 180 days.


3. 

```{python}
import matplotlib.pyplot as plt
import altair as alt

pos2017 = pd.read_csv(
    '~/Desktop/problem-set-4-xy-wz/pos2017.csv', encoding='latin1')
pos2018 = pd.read_csv(
    '~/Desktop/problem-set-4-xy-wz/pos2018.csv', encoding='latin1')
pos2019 = pd.read_csv(
    '~/Desktop/problem-set-4-xy-wz/pos2019.csv', encoding='latin1')

short_term_hospitals_2017 = pos2017[(pos2017['PRVDR_CTGRY_CD'] == 1) & (
    pos2017['PRVDR_CTGRY_SBTYP_CD'] == 1)]
short_term_hospitals_2018 = pos2018[(pos2018['PRVDR_CTGRY_CD'] == 1) & (
    pos2018['PRVDR_CTGRY_SBTYP_CD'] == 1)]
short_term_hospitals_2019 = pos2019[(pos2019['PRVDR_CTGRY_CD'] == 1) & (
    pos2019['PRVDR_CTGRY_SBTYP_CD'] == 1)]

all_years = pd.concat([short_term_hospitals_2016, short_term_hospitals_2017, short_term_hospitals_2018, short_term_hospitals_2019], keys=[
                      '2016', '2017', '2018', '2019']).reset_index(level=0).rename(columns={'level_0': 'Year'})

observations_per_year = all_years.groupby(
    'Year').size().reset_index(name='Number_of_Hospitals')

chart_observations = alt.Chart(observations_per_year).mark_bar().encode(
    alt.X('Year:O', title='Year'),
    alt.Y('Number_of_Hospitals:Q', title='Number of Hospitals'),
    tooltip=['Year', 'Number_of_Hospitals']
).properties(
    title='Number of Short-Term Hospitals by Year (2016-2019)',
    width=400,
    height=250
)

chart_observations.display()

num_hospitals_2017 = short_term_hospitals_2017.shape[0]
num_hospitals_2018 = short_term_hospitals_2018.shape[0]
num_hospitals_2019 = short_term_hospitals_2019.shape[0]

print(f"Number of short-term hospitals in 2017: {num_hospitals_2017}")
print(f"Number of short-term hospitals in 2018: {num_hospitals_2018}")
print(f"Number of short-term hospitals in 2019: {num_hospitals_2019}")


```


4. 
    a.
```{python}

unique_hospitals_per_year = all_years.groupby('Year')['PRVDR_NUM'].nunique().reset_index()
unique_hospitals_per_year.columns = ['Year', 'Unique_Hospitals']

chart_unique_per_year = alt.Chart(unique_hospitals_per_year).mark_bar().encode(
    alt.X('Year:O', title='Year'),
    alt.Y('Unique_Hospitals:Q', title='Number of Unique Hospitals'),
    tooltip=['Year', 'Unique_Hospitals']
).properties(
    title='Number of Unique Short-Term Hospitals by Year (2016-2019)',
    width=400,
    height=250
)

chart_unique_per_year.display()

```

    b.The comparison between the total number of hospitals and the number of unique hospitals reveals that the data is highly consistent across the years. In both plots, the number of hospitals remains almost unchanged from 2016 to 2019, with the total number of hospitals and unique hospitals being very close to each other. This suggests that most hospitals were consistently present throughout the time period, indicating minimal new entries or exits from the dataset. The stability in both total and unique counts highlights that the healthcare system had little fluctuation in terms of hospital availability, pointing to a continuous and stable system over these years.


## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}

file_paths = ["pos2016.csv", "pos2017.csv", "pos2018.csv", "pos2019.csv"]

dataframes = []
for file_path in file_paths:
    try:
        df = pd.read_csv(file_path, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(file_path, encoding="ISO-8859-1")
    dataframes.append(df)

active_code = 0
short_term_category_code = 1
short_term_subtype_code = 1

active_hospitals_2016 = dataframes[0][
    (dataframes[0]["PGM_TRMNTN_CD"] == active_code)
    & (dataframes[0]["PRVDR_CTGRY_CD"] == short_term_category_code)
    & (dataframes[0]["PRVDR_CTGRY_SBTYP_CD"] == short_term_subtype_code)
]

suspected_closures = []

merged_df = active_hospitals_2016.merge(
    dataframes[3], on="PRVDR_NUM", how="left", suffixes=("_2016", "_2019")
)

suspected_closures_df = merged_df[
    merged_df["PGM_TRMNTN_CD_2019"].isna()
    | (merged_df["PGM_TRMNTN_CD_2019"] != active_code)
]

for _, row in suspected_closures_df.iterrows():
    suspected_closures.append(
        {
            "Facility Name": row["FAC_NAME_2016"],
            "ZIP Code": row["ZIP_CD_2016"],
            "Year of Suspected Closure": 2019,
            "Type of Reason": 1,
        }
    )

suspected_closures_df = pd.DataFrame(suspected_closures)

num_suspected_closures_2019 = suspected_closures_df.shape[0]
print(
    f"Number of short-term hospitals suspected to have closed by 2019: {num_suspected_closures_2019}"
)

```

2. 

```{python}
sorted_suspected_closures_df = suspected_closures_df.sort_values(
    by="Facility Name"
).head(10)
print(
    sorted_suspected_closures_df[
        ["Facility Name", "Year of Suspected Closure", "Type of Reason"]
    ]
)
```

3. 
    a.

```{python}

zip_code_counts_2016 = active_hospitals_2016["ZIP_CD"].value_counts()
zip_code_counts_2019 = dataframes[3][
    (dataframes[3]["PRVDR_CTGRY_CD"] == short_term_category_code)
    & (dataframes[3]["PRVDR_CTGRY_SBTYP_CD"] == short_term_subtype_code)
    & (dataframes[3]["PGM_TRMNTN_CD"] == active_code)
]["ZIP_CD"].value_counts()

filtered_suspected_closures = []
merger_candidates = []
for _, row in merged_df.iterrows():
    if pd.isna(row["PGM_TRMNTN_CD_2019"]) or row["PGM_TRMNTN_CD_2019"] != active_code:
        zip_code = row["ZIP_CD_2016"]
        if zip_code in zip_code_counts_2016 and zip_code in zip_code_counts_2019:
            if zip_code_counts_2019[zip_code] < zip_code_counts_2016[zip_code]:
                filtered_suspected_closures.append(row)
            elif row["PGM_TRMNTN_CD_2019"] == 1:
                merger_candidates.append(row)
        elif zip_code in zip_code_counts_2016 and zip_code not in zip_code_counts_2019:
            filtered_suspected_closures.append(row)

filtered_suspected_closures_df = pd.DataFrame(filtered_suspected_closures)
merger_candidates_df = pd.DataFrame(merger_candidates)

num_merger_candidates = merger_candidates_df.shape[0]
print(
    f"Number of hospitals potentially involved in a merger/acquisition: {num_merger_candidates}"
)

num_filtered_suspected_closures = filtered_suspected_closures_df.shape[0]
print(
    f"Filtered number of short-term hospitals suspected to have closed by 2019: {num_filtered_suspected_closures}"
)

```

    b.

```{python}
# Question 3b: Display the number of hospitals left after correcting for potential mergers/acquisitions
num_filtered_suspected_closures = filtered_suspected_closures_df.shape[0]
print(f"Number of hospitals left after correcting for potential mergers/acquisitions: {num_filtered_suspected_closures}")

```
    
    c.

```{python}
filtered_suspected_closures_df["Year of Suspected Closure"] = 2019

sorted_filtered_closures = filtered_suspected_closures_df.sort_values(
    by="FAC_NAME_2016"
).head(10)

print(
    sorted_filtered_closures[
        ["FAC_NAME_2016", "ZIP_CD_2016", "Year of Suspected Closure"]
    ]
)
```

## Download Census zip code shapefile (10 pt) 

1. 
    a. The .dbf file is a database file that stores attribute data for each geographical feature, similar to a spreadsheet, which allows you to access information like names, zip codes, or other attributes associated with each shape. The .prj file contains the projection information, which defines the coordinate system used by the shapefile. This is crucial for aligning the data with other geographic datasets properly. The .shp file is the main shape file and stores the actual geometry, including the coordinates that define the spatial features like points, lines, and polygons. The .shx file is an index file that provides a positional index of the geometries in the .shp file, enabling faster data access and locating features within the dataset. Lastly, the .xml file contains metadata, which offers descriptions and additional information about the shapefile, such as field definitions and projection details, helping to understand the dataset more effectively.
    b. 

    ```{python}

    import os
    
<<<<<<< HEAD
    directory_path = '/Users/hahei/Desktop/gz_2010_us_860_00_500k'
=======
    directory_path = '~/Desktop/gz_2010_us_860_00_500k‘
>>>>>>> 20066b65cce4d21811238ffc904f4822bd99aba3
    
    file_names = [
    'gz_2010_us_860_00_500k.dbf',
    'gz_2010_us_860_00_500k.prj',
    'gz_2010_us_860_00_500k.shp',
    'gz_2010_us_860_00_500k.shx',
    'gz_2010_us_860_00_500k.xml'
    ]
    
    for file in file_names:
        file_path = os.path.join(directory_path, file)
        
        if os.path.exists(file_path): 
            size_bytes = os.path.getsize(file_path)
            size_mb = size_bytes / (1024 * 1024)
            print(f"{file}: {size_mb:.2f} MB")
        else:
            print(f"File not found: {file_path}")
    
    ```

2. 

```{python}
import matplotlib.pyplot as plt
import geopandas as gpd

<<<<<<< HEAD
base_path = "/Users/hahei/Desktop/gz_2010_us_860_00_500k"

pathshp = os.path.join(base_path, "gz_2010_us_860_00_500k.shp")
=======
pathshp = ‘~/Desktop/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp’
>>>>>>> 20066b65cce4d21811238ffc904f4822bd99aba3
df_shp = gpd.read_file(pathshp)

df_texas = df_shp[df_shp["ZCTA5"].str.startswith(("75", "76", "77", "78", "79"))]
df_texas["ZCTA5"] = df_texas["ZCTA5"].astype(str)

hospital_2016 = pos2016[
    (pos2016["PRVDR_CTGRY_CD"] == 1) & (pos2016["PRVDR_CTGRY_SBTYP_CD"] == 1)
]

hospital_2016["ZIP_CD"] = (
    hospital_2016["ZIP_CD"]
    .astype(str)
    .str.replace(r"\.0$", "", regex=True)
    .str.zfill(5)
)

hospital_zip = hospital_2016.groupby("ZIP_CD").size().reset_index(name="hospital_count")

map_texas = df_texas.merge(hospital_zip, left_on="ZCTA5", right_on="ZIP_CD", how="left")
map_texas["hospital_count"] = map_texas["hospital_count"].fillna(0)

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
map_texas.plot(
    column="hospital_count",
    cmap="viridis",
    linewidth=0.8,
    ax=ax,
    edgecolor="0.8",
    legend=True,
)
ax.set_title("Number of Hospitals per ZIP Code in Texas (2016)", fontsize=15)
plt.axis("off")
plt.show()

```


## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 

```{python}
base_path = "/Users/hahei/Desktop/gz_2010_us_860_00_500k"
shapefile_path = os.path.join(base_path, "gz_2010_us_860_00_500k.shp")

gdf = gpd.read_file(shapefile_path)

zips_all_centroids = gdf.copy()
zips_all_centroids["geometry"] = gdf.centroid

print("GeoDataFrame dimensionality:", zips_all_centroids.shape)
print("GeoDataFrame Column information:")
print(zips_all_centroids.columns)

print(zips_all_centroids.head())

```


2. 

```{python}
texas_prefixes = ("75", "76", "77", "78", "79")

zips_texas_centroids = zips_all_centroids[
    zips_all_centroids["ZCTA5"].str.startswith(texas_prefixes)
]
num_unique_texas_zips = zips_texas_centroids["ZCTA5"].nunique()
print(f"Number of unique zip codes in Texas: {num_unique_texas_zips}")

bordering_state_prefixes = (
    "70",
    "71",
    "72",
    "73",
    "74",
    "75",
    "76",
    "77",
    "78",
    "79",
    "87",
    "88",
)

zips_texas_borderstates_centroids = zips_all_centroids[
    zips_all_centroids["ZCTA5"].str.startswith(bordering_state_prefixes)
]
num_unique_borderstate_zips = zips_texas_borderstates_centroids["ZCTA5"].nunique()
print(
    f"Number of unique zip codes in Texas and bordering states: {num_unique_borderstate_zips}"
)

```

3. 

```{python}
pos2016_path = "pos2016.csv"
pos2016_df = pd.read_csv(pos2016_path)

short_term_category_code = 1
short_term_subtype_code = 1
active_code = 0

active_hospitals_2016 = pos2016_df[
    (pos2016_df['PRVDR_CTGRY_CD'] == short_term_category_code) &
    (pos2016_df['PRVDR_CTGRY_SBTYP_CD'] == short_term_subtype_code) &
    (pos2016_df['PGM_TRMNTN_CD'] == active_code)
]

active_hospitals_2016 = active_hospitals_2016.copy()

active_hospitals_2016['ZIP_CD'] = active_hospitals_2016['ZIP_CD'].fillna(0).astype(int).astype(str).str.zfill(5)

hospital_counts = active_hospitals_2016['ZIP_CD'].value_counts().reset_index()
hospital_counts.columns = ['ZCTA5', 'hospital_count']

zips_with_hospitals = hospital_counts[hospital_counts['hospital_count'] >= 1]

zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    zips_with_hospitals, on='ZCTA5', how='inner'
)

print(zips_withhospital_centroids.head())
print(f"Number of zip codes with at least 1 hospital in 2016: {zips_withhospital_centroids.shape[0]}")

```


4. 
    a.

```{python}

zips_texas_centroids = zips_texas_centroids.to_crs(epsg=3857)
zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=3857)

test_zips = zips_texas_centroids.head(10)

start_time = time.time()

distances = []

for _, test_zip in test_zips.iterrows():
    test_geometry = test_zip["geometry"]

    min_distance = zips_withhospital_centroids["geometry"].distance(test_geometry).min()

    distances.append(
        {"ZCTA5": test_zip["ZCTA5"], "Nearest_Hospital_Distance": min_distance}
    )

end_time = time.time()

execution_time = end_time - start_time
print(f"Time taken for 10 zip codes: {execution_time:.2f} seconds")

distance_results = pd.DataFrame(distances)
print(distance_results)

total_zip_codes = len(zips_texas_centroids)
estimated_time = (execution_time / 10) * total_zip_codes
print(f"Estimated time for all zip codes in Texas: {estimated_time / 60:.2f} minutes")

```

    b.

```{python}

start_time = time.time()

all_distances = []

for _, texas_zip in zips_texas_centroids.iterrows():
    texas_geometry = texas_zip["geometry"]

    min_distance = (
        zips_withhospital_centroids["geometry"].distance(texas_geometry).min()
    )

    all_distances.append(
        {"ZCTA5": texas_zip["ZCTA5"], "Nearest_Hospital_Distance": min_distance}
    )

end_time = time.time()

execution_time = end_time - start_time
print(f"Time taken for all zip codes in Texas: {execution_time:.2f} seconds")

all_distance_results = pd.DataFrame(all_distances)
print(all_distance_results.head())

all_distance_results.to_csv("texas_zipcode_hospital_distances.csv", index=False)

estimated_time = (0.08 / 10) * len(zips_texas_centroids)
print(f"Estimated time for all zip codes in Texas: {estimated_time:.2f} seconds")

if execution_time <= estimated_time:
    print("Actual execution time is less than or close to the estimated time.")
else:
    print("Actual execution time is greater than the estimated time.")

```

5. 
    a.
    ### Ans: The unit in the .prj file is Degree.


    b.

```{python}

all_distance_results["Nearest_Hospital_Distance_Miles"] = (
    all_distance_results["Nearest_Hospital_Distance"] * conversion_factor
)

average_distance_miles = all_distance_results["Nearest_Hospital_Distance_Miles"].mean()
print(
    f"Average distance to the nearest hospital in Texas (in miles): {average_distance_miles:.2f} miles"
)

if average_distance_miles < 30:
    print(
        "This value makes sense, considering the distribution of hospitals and rural areas in Texas."
    )
else:
    print(
        "This value seems unusually high, possibly indicating a lack of hospitals in some areas."
    )

```

    c.

```{python}

base_path = "/Users/hahei/Desktop/gz_2010_us_860_00_500k"
pathshp = os.path.join(base_path, "gz_2010_us_860_00_500k.shp")
df_shp = gpd.read_file(pathshp)

df_texas = df_shp[df_shp["ZCTA5"].str.startswith(("75", "76", "77", "78", "79"))]
df_texas["ZCTA5"] = df_texas["ZCTA5"].astype(str)

df_texas_distance = df_texas.merge(
    all_distance_results[["ZCTA5", "Nearest_Hospital_Distance_Miles"]],
    on="ZCTA5",
    how="left"
)

df_texas_distance["Nearest_Hospital_Distance_Miles"] = df_texas_distance["Nearest_Hospital_Distance_Miles"].fillna(0)

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
df_texas_distance.plot(
    column="Nearest_Hospital_Distance_Miles",
    cmap="OrRd",  
    linewidth=0.8,
    ax=ax,
    edgecolor="0.8",
    legend=True,
)

ax.set_title("Distance to Nearest Hospital by Zip Code in Texas (in Miles)", fontsize=15)
plt.axis("off")

plt.show()
```
    
## Effects of closures on access in Texas (15 pts)

1. 

```{python}

file_paths = ["pos2016.csv", "pos2017.csv", "pos2018.csv", "pos2019.csv"]

dataframes = [pd.read_csv(file_path, encoding='latin1') for file_path in file_paths]
pos_data = pd.concat(dataframes, ignore_index=True)

pos_data = pos_data.drop_duplicates(subset=['PRVDR_NUM', 'PGM_TRMNTN_CD'])

closures_2016_2019 = pos_data[(pos_data['PRVDR_CTGRY_CD'] == 1) & (pos_data['PGM_TRMNTN_CD'] == 1)].copy()
closures_2016_2019['ZIP_CD'] = closures_2016_2019['ZIP_CD'].astype(str).str.replace(r'\.0$', '', regex=True).str.zfill(5)
closures_texas = closures_2016_2019[closures_2016_2019['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79'))].copy()

closures_zip = closures_texas.groupby('ZIP_CD')['PRVDR_NUM'].nunique().reset_index(name='closure_count')

zip_closure_table = closures_zip.groupby('closure_count').size().reset_index(name='number_of_zip_codes')
print(zip_closure_table)

```

2. 

```{python}

```

3. 

```{python}

```

4. 

```{python}

```

## Reflecting on the exercise (10 pts) 
